{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":99552,"databundleVersionId":13851420,"sourceType":"competition"},{"sourceId":13735721,"sourceType":"datasetVersion","datasetId":8739635}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 1. Copy the entire code repository from our \"code dataset\"\n#    This path matches your new setup.\n!cp -r /kaggle/input/rsna2025/rsna2025 /kaggle/working/rsna_project\n\n# 2. Move into our new project directory\n%cd /kaggle/working/rsna_project\n\n# 3. Install system dependencies (dcm2niix)\nprint(\"--- Installing dcm2niix ---\")\n!apt-get update\n!apt-get install -y dcm2niix\n\n# 4. Install all Python packages from the requirements file\nprint(\"\\n--- Installing Python packages ---\")\n!pip install -r pip_packages/requirements.txt\n\n# 5. Install the custom nnU-Net\nprint(\"\\n--- Installing local nnU-Net ---\")\n!pip install -e ./nnUNet\n\nprint(\"\\n✅ Setup complete! All code is local and all packages are installed.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We must be in this directory for the scripts to work\n%cd /kaggle/working/rsna_project","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T20:23:19.672925Z","iopub.execute_input":"2025-11-14T20:23:19.673586Z","iopub.status.idle":"2025-11-14T20:23:19.678475Z","shell.execute_reply.started":"2025-11-14T20:23:19.673563Z","shell.execute_reply":"2025-11-14T20:23:19.677673Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/rsna_project\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# --- Create the directory structure ---\n!mkdir -p /workspace/data\n!mkdir -p /workspace/data/nnUNet/nnUNet_raw\n!mkdir -p /workspace/data/nnUNet/nnUNet_preprocessed\n!mkdir -p /workspace/data/nnUNet/nnUNet_results\n\n# --- Link Competition Data into /workspace/data ---\n!ln -s /kaggle/input/rsna-intracranial-aneurysm-detection/series /workspace/data/series\n!ln -s /kaggle/input/rsna-intracranial-aneurysm-detection/segmentations /workspace/data/segmentations\n!ln -s /kaggle/input/rsna-intracranial-aneurysm-detection/train_localizers.csv /workspace/data/train_localizers.csv\n\n# --- Copy the error file the script was looking for ---\n!cp /kaggle/working/rsna_project/data/error_data.yaml /workspace/data/error_data.yaml\n\n# --- Set the environment variables to this new location ---\n%env nnUNet_raw=\"/workspace/data/nnUNet/nnUNet_raw\"\n%env nnUNet_preprocessed=\"/workspace/data/nnUNet/nnUNet_preprocessed\"\n%env nnUNet_results=\"/workspace/data/nnUNet/nnUNet_results\"\n\nprint(\"✅ Fixed environment! '/workspace/data' is now set up.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T20:32:02.848363Z","iopub.execute_input":"2025-11-14T20:32:02.849050Z","iopub.status.idle":"2025-11-14T20:32:03.782373Z","shell.execute_reply.started":"2025-11-14T20:32:02.849021Z","shell.execute_reply":"2025-11-14T20:32:03.781582Z"}},"outputs":[{"name":"stdout","text":"env: nnUNet_raw=\"/workspace/data/nnUNet/nnUNet_raw\"\nenv: nnUNet_preprocessed=\"/workspace/data/nnUNet/nnUNet_preprocessed\"\nenv: nnUNet_results=\"/workspace/data/nnUNet/nnUNet_results\"\n✅ Fixed environment! '/workspace/data' is now set up.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# 1. Load the REAL training file from the competition data\nfull_train_df = pd.read_csv('/kaggle/input/rsna-intracranial-aneurysm-detection/train.csv')\n\n# 2. Get a list of all unique scan IDs\nall_scan_ids = full_train_df['SeriesInstanceUID'].unique()\nprint(f\"Total scans in dataset: {len(all_scan_ids)}\")\n\n# 3. Select a small sample of scans (e.g., 100)\nnp.random.seed(42) # for reproducible results\nsample_scan_ids = np.random.choice(all_scan_ids, 100, replace=False)\n\n# 4. Filter the main dataframe to ONLY include these 100 scans\nsample_df = full_train_df[full_train_df['SeriesInstanceUID'].isin(sample_scan_ids)]\n\n# 5. Save this SMALL dataframe to the location the code expects\nsample_df.to_csv('/workspace/data/train.csv', index=False)\n\nprint(f\"✅ Created a sample 'train.csv' with {len(sample_scan_ids)} scans at /workspace/data/.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T20:32:09.967939Z","iopub.execute_input":"2025-11-14T20:32:09.968242Z","iopub.status.idle":"2025-11-14T20:32:09.995382Z","shell.execute_reply.started":"2025-11-14T20:32:09.968216Z","shell.execute_reply":"2025-11-14T20:32:09.994807Z"}},"outputs":[{"name":"stdout","text":"Total scans in dataset: 4348\n✅ Created a sample 'train.csv' with 5 scans at /workspace/data/.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"pd.read_csv('/kaggle/input/rsna-intracranial-aneurysm-detection/train.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T20:24:31.317540Z","iopub.execute_input":"2025-11-14T20:24:31.317849Z","iopub.status.idle":"2025-11-14T20:24:31.357316Z","shell.execute_reply.started":"2025-11-14T20:24:31.317825Z","shell.execute_reply":"2025-11-14T20:24:31.356492Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                      SeriesInstanceUID  PatientAge  \\\n0     1.2.826.0.1.3680043.8.498.10004044428023505108...          64   \n1     1.2.826.0.1.3680043.8.498.10004684224894397679...          76   \n2     1.2.826.0.1.3680043.8.498.10005158603912009425...          58   \n3     1.2.826.0.1.3680043.8.498.10009383108068795488...          71   \n4     1.2.826.0.1.3680043.8.498.10012790035410518400...          48   \n...                                                 ...         ...   \n4343  1.2.826.0.1.3680043.8.498.99915610493694667606...          62   \n4344  1.2.826.0.1.3680043.8.498.99920680741054836990...          76   \n4345  1.2.826.0.1.3680043.8.498.99953513260518059135...          44   \n4346  1.2.826.0.1.3680043.8.498.99982144859397209076...          58   \n4347  1.2.826.0.1.3680043.8.498.99985209798463601651...          65   \n\n     PatientSex    Modality  Left Infraclinoid Internal Carotid Artery  \\\n0        Female         MRA                                          0   \n1        Female         MRA                                          0   \n2          Male         CTA                                          0   \n3          Male         MRA                                          0   \n4        Female         MRA                                          0   \n...         ...         ...                                        ...   \n4343     Female  MRI T1post                                          0   \n4344     Female         MRA                                          0   \n4345     Female         CTA                                          0   \n4346     Female      MRI T2                                          0   \n4347     Female         CTA                                          0   \n\n      Right Infraclinoid Internal Carotid Artery  \\\n0                                              0   \n1                                              0   \n2                                              0   \n3                                              0   \n4                                              0   \n...                                          ...   \n4343                                           0   \n4344                                           0   \n4345                                           0   \n4346                                           0   \n4347                                           0   \n\n      Left Supraclinoid Internal Carotid Artery  \\\n0                                             0   \n1                                             0   \n2                                             0   \n3                                             0   \n4                                             0   \n...                                         ...   \n4343                                          0   \n4344                                          0   \n4345                                          0   \n4346                                          0   \n4347                                          0   \n\n      Right Supraclinoid Internal Carotid Artery  Left Middle Cerebral Artery  \\\n0                                              0                            0   \n1                                              0                            0   \n2                                              0                            0   \n3                                              0                            0   \n4                                              0                            0   \n...                                          ...                          ...   \n4343                                           0                            0   \n4344                                           0                            0   \n4345                                           0                            0   \n4346                                           0                            0   \n4347                                           0                            1   \n\n      Right Middle Cerebral Artery  Anterior Communicating Artery  \\\n0                                0                              0   \n1                                0                              0   \n2                                0                              0   \n3                                0                              0   \n4                                0                              0   \n...                            ...                            ...   \n4343                             0                              0   \n4344                             0                              0   \n4345                             0                              0   \n4346                             0                              0   \n4347                             0                              0   \n\n      Left Anterior Cerebral Artery  Right Anterior Cerebral Artery  \\\n0                                 0                               0   \n1                                 0                               0   \n2                                 0                               0   \n3                                 0                               0   \n4                                 0                               0   \n...                             ...                             ...   \n4343                              0                               0   \n4344                              0                               0   \n4345                              0                               0   \n4346                              0                               0   \n4347                              0                               0   \n\n      Left Posterior Communicating Artery  \\\n0                                       0   \n1                                       0   \n2                                       0   \n3                                       0   \n4                                       0   \n...                                   ...   \n4343                                    0   \n4344                                    0   \n4345                                    0   \n4346                                    0   \n4347                                    0   \n\n      Right Posterior Communicating Artery  Basilar Tip  \\\n0                                        0            0   \n1                                        0            0   \n2                                        0            0   \n3                                        0            0   \n4                                        0            0   \n...                                    ...          ...   \n4343                                     0            0   \n4344                                     0            0   \n4345                                     0            0   \n4346                                     0            0   \n4347                                     0            0   \n\n      Other Posterior Circulation  Aneurysm Present  \n0                               0                 0  \n1                               0                 0  \n2                               1                 1  \n3                               0                 0  \n4                               0                 0  \n...                           ...               ...  \n4343                            0                 0  \n4344                            0                 0  \n4345                            0                 0  \n4346                            0                 0  \n4347                            0                 1  \n\n[4348 rows x 18 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SeriesInstanceUID</th>\n      <th>PatientAge</th>\n      <th>PatientSex</th>\n      <th>Modality</th>\n      <th>Left Infraclinoid Internal Carotid Artery</th>\n      <th>Right Infraclinoid Internal Carotid Artery</th>\n      <th>Left Supraclinoid Internal Carotid Artery</th>\n      <th>Right Supraclinoid Internal Carotid Artery</th>\n      <th>Left Middle Cerebral Artery</th>\n      <th>Right Middle Cerebral Artery</th>\n      <th>Anterior Communicating Artery</th>\n      <th>Left Anterior Cerebral Artery</th>\n      <th>Right Anterior Cerebral Artery</th>\n      <th>Left Posterior Communicating Artery</th>\n      <th>Right Posterior Communicating Artery</th>\n      <th>Basilar Tip</th>\n      <th>Other Posterior Circulation</th>\n      <th>Aneurysm Present</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.2.826.0.1.3680043.8.498.10004044428023505108...</td>\n      <td>64</td>\n      <td>Female</td>\n      <td>MRA</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.2.826.0.1.3680043.8.498.10004684224894397679...</td>\n      <td>76</td>\n      <td>Female</td>\n      <td>MRA</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.2.826.0.1.3680043.8.498.10005158603912009425...</td>\n      <td>58</td>\n      <td>Male</td>\n      <td>CTA</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.2.826.0.1.3680043.8.498.10009383108068795488...</td>\n      <td>71</td>\n      <td>Male</td>\n      <td>MRA</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.2.826.0.1.3680043.8.498.10012790035410518400...</td>\n      <td>48</td>\n      <td>Female</td>\n      <td>MRA</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4343</th>\n      <td>1.2.826.0.1.3680043.8.498.99915610493694667606...</td>\n      <td>62</td>\n      <td>Female</td>\n      <td>MRI T1post</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4344</th>\n      <td>1.2.826.0.1.3680043.8.498.99920680741054836990...</td>\n      <td>76</td>\n      <td>Female</td>\n      <td>MRA</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4345</th>\n      <td>1.2.826.0.1.3680043.8.498.99953513260518059135...</td>\n      <td>44</td>\n      <td>Female</td>\n      <td>CTA</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4346</th>\n      <td>1.2.826.0.1.3680043.8.498.99982144859397209076...</td>\n      <td>58</td>\n      <td>Female</td>\n      <td>MRI T2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4347</th>\n      <td>1.2.826.0.1.3680043.8.498.99985209798463601651...</td>\n      <td>65</td>\n      <td>Female</td>\n      <td>CTA</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>4348 rows × 18 columns</p>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"!ls -R /workspace","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T20:24:48.582281Z","iopub.execute_input":"2025-11-14T20:24:48.583028Z","iopub.status.idle":"2025-11-14T20:24:48.705005Z","shell.execute_reply.started":"2025-11-14T20:24:48.583002Z","shell.execute_reply":"2025-11-14T20:24:48.704228Z"}},"outputs":[{"name":"stdout","text":"ls: cannot access '/workspace': No such file or directory\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import SimpleITK as sitk\nimport pydicom\nimport joblib\nimport subprocess\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport os\nimport json\n\n# --- 0. SETUP & SANITY CHECKS ---\n# Ensure we are in the correct directory\n%cd /kaggle/working/rsna_project\nprint(f\"Current directory: {os.getcwd()}\")\n\n# Force-set environment variables again\nos.environ['nnUNet_raw'] = \"/workspace/data/nnUNet/nnUNet_raw\"\nos.environ['nnUNet_preprocessed'] = \"/workspace/data/nnUNet/nnUNet_preprocessed\"\nos.environ['nnUNet_results'] = \"/workspace/data/nnUNet/nnUNet_results\"\nprint(\"✅ Environment variables set.\")\n\n# --- 1/8: DICOM CONVERSION (ROBUST VERSION) ---\nprint(\"\\n--- 1/8: Converting DICOM to NIfTI ---\")\n\n# 1. Define the config \nclass CFG:\n    img_dir = Path(\"/workspace/data/series\")\n    out_dir = Path(\"/workspace/data/series_niix\")\n    csv_dir = Path(\"/workspace/data\")\n    error_dir = Path(\"/workspace/data/error_data\")\n    num_workers = os.cpu_count()\n    \n# 2. Define the conversion functions \ndef dcm2niix(src_path: Path, dst_path: Path, series_id: str) -> int:\n    dst_path.mkdir(parents=True, exist_ok=True)\n    cmd = [\"dcm2niix\", \"-o\", str(dst_path), \"-f\", f\"{series_id}\", \"-z\", \"y\", \"-b\", \"n\", \"-m\", \"2\", str(src_path)]\n    try:\n        subprocess.run(cmd, check=True, capture_output=True, text=True, timeout=300)\n    except (subprocess.CalledProcessError, subprocess.TimeoutExpired):\n        return 1\n    return 0\n\ndef run_dcm2niix(series_ids: list[str]):\n    print(f\"Running dcm2niix for {len(series_ids)} series...\")\n    results = joblib.Parallel(n_jobs=CFG.num_workers)(\n        joblib.delayed(dcm2niix)(\n            src_path=CFG.img_dir / series_id,\n            dst_path=CFG.out_dir / series_id,\n            series_id=series_id,\n        )\n        for series_id in tqdm(series_ids, desc=\"Converting\")\n    )\n    success_count = sum(1 for res in results if res == 0)\n    print(f\"Done: {success_count} success, {len(results) - success_count} failure\")\n\n# 3. Run the conversion on OUR 100 scans\ntry:\n    sample_df = pd.read_csv('/workspace/data/train.csv')\n    series_ids_to_process = sample_df['SeriesInstanceUID'].tolist()\n    print(f\"Found {len(series_ids_to_process)} scans to process.\")\n    run_dcm2niix(series_ids_to_process)\n    \n    print(\"\\n✅ DICOM conversion finished.\")\nexcept Exception as e:\n    print(f\"❌ An error occurred during conversion: {e}\")\n\n# --- 2/8: Moving error data ---\nprint(\"\\n--- 2/8: Moving error data ---\")\n!python src/my_utils/move_error_data.py\n\n# --- 3/8: Creating nnU-Net Datasets ---\nprint(\"\\n--- 3/8: Creating nnU-Net Dataset 001 & 003 ---\")\n!python src/nnUnet_utils/create_nnunet_dataset.py\n!python src/nnUnet_utils/create_nnunet_dataset.py --dataset-id 3\n\n# --- 4/8: Planning & Preprocessing Dataset 001 (RAM FIX) ---\nprint(\"\\n--- 4/8: Planning and preprocessing Dataset 001 (3d_fullres only) ---\")\n# We use -c 3d_fullres to skip the 2d config that crashes RAM\n!nnUNetv2_plan_and_preprocess -d 1 --verify_dataset_integrity -pl nnUNetPlannerResEncM -c 3d_fullres -np 2\nprint(\"✅ Preprocessing for Dataset 001 complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T20:32:15.268166Z","iopub.execute_input":"2025-11-14T20:32:15.268814Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/rsna_project\nCurrent directory: /kaggle/working/rsna_project\n✅ Environment variables set.\n\n--- 1/8: Converting DICOM to NIfTI ---\nFound 5 scans to process.\nRunning dcm2niix for 5 series...\n","output_type":"stream"},{"name":"stderr","text":"Converting: 100%|██████████| 5/5 [00:00<00:00, 2944.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Done: 4 success, 1 failure\n\n✅ DICOM conversion finished.\n\n--- 2/8: Moving error data ---\n2025-11-14 20:32:24,499 - INFO - Error series count: 57\n2025-11-14 20:32:24,499 - INFO - Backup root: /workspace/data/series_niix_error_data_backup\nMoving error data: 100%|█████████████████████| 57/57 [00:00<00:00, 12679.68it/s]\n2025-11-14 20:32:24,514 - INFO - Moved: 57\n2025-11-14 20:32:24,514 - INFO - Not found: 0\n2025-11-14 20:32:24,515 - INFO - Backup verified: 57/57\n\n==================================================\nFinished moving error data\n==================================================\nTotal targets: 57\nMoved: 57\nNot found: 0\nBackup root: /workspace/data/series_niix_error_data_backup\n==================================================\n\n--- 3/8: Creating nnU-Net Dataset 001 & 003 ---\nUsing dataset ID 1 (binarization: Disabled / class remapping: Disabled)\nOutput directory: /workspace/data/nnUNet/nnUNet_raw/Dataset001_VesselSegmentation\nNumber of cases to process: 178\nProcessing: 100%|█████████████████████████████| 178/178 [25:00<00:00,  8.43s/it]\n\nDone:\n  Success: 178/178 cases\n  Modality distribution: {'CT': 74, 'MR': 103, 'Unknown': 1}\nMultiple modalities detected: {'CT': 74, 'MR': 103, 'Unknown': 1}\nUsing MR normalization mode\nCreated dataset.json: /workspace/data/nnUNet/nnUNet_raw/Dataset001_VesselSegmentation/dataset.json\n\nCreated dataset at: /workspace/data/nnUNet/nnUNet_raw/Dataset001_VesselSegmentation\nCase mapping file: /workspace/data/nnUNet/nnUNet_raw/Dataset001_VesselSegmentation/case_mapping.json\n\nNext steps:\n1. Set nnUNet environment variables:\n   export nnUNet_raw=/workspace/nnUNet_raw\n   export nnUNet_preprocessed=/workspace/nnUNet_preprocessed\n   export nnUNet_results=/workspace/nnUNet_results\n2. Run preprocessing:\n   nnUNetv2_plan_and_preprocess -d 1 --verify_dataset_integrity\n3. Start training:\n   nnUNetv2_train 1 3d_fullres 0\nUsing dataset ID 3 (binarization: Disabled / class remapping: Enabled)\nOutput directory: /workspace/data/nnUNet/nnUNet_raw/Dataset003_VesselGrouping\nNumber of cases to process: 178\nProcessing:  48%|██████████████▍               | 86/178 [21:43<28:45, 18.75s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# --- 5/8: Planning & Preprocessing Dataset 003 (CORRECT COMMAND) ---\nprint(\"\\n--- 5/8: Planning and preprocessing Dataset 003 ---\")\n# This is the correct command to create the '...ForcedLowres' plan\n!nnUNetv2_plan_and_preprocess -d 3 --verify_dataset_integrity -pl nnUNetPlannerResEncMForcedLowres -overwrite_target_spacing 1.0 1.0 1.0 -c 3d_fullres -np 2\nprint(\"✅ Planning and Preprocessing for Dataset 003 complete.\")\n\n# --- 6/8: JSON Patch (CORRECT FILE NAME) ---\nprint(\"\\n--- 6/8: Re-running JSON patch with correct file name ---\")\nplan_file = \"/workspace/data/nnUNet/nnUNet_preprocessed/Dataset003_VesselGrouping/nnUNetResEncUNetMPlans_ForcedLowres.json\"\n\ntry:\n    with open(plan_file, 'r') as f: data = json.load(f)\n    \n    data['plans_per_stage'][0]['patch_size'] = [64, 64, 64]\n    \n    with open(plan_file, 'w') as f: json.dump(data, f, indent=4)\n    print(\"✅ Patched nnUNet plans.json successfully.\")\n    \n    # 6.2 Re-preprocess Dataset 003 with the new patch size\n    print(\"\\n--- 6.2: Re-preprocessing Dataset 003 with new patch size ---\")\n    !nnUNetv2_preprocess -d 3 -plans_name nnUNetResEncUNetMPlans_ForcedLowres -c 3d_fullres -np 2\n    print(\"✅ Re-preprocessing for Dataset 003 complete.\")\nexcept Exception as e:\n    print(f\"❌ Error patching JSON. The file '{plan_file}' was not found. {e}\")\n\n# --- 7/8 & 8/8: Final Steps ---\nprint(\"\\n--- 7/8: Creating inference dataset ---\")\n!python src/nnUnet_utils/create_nnunet_inference_dataset.py\nprint(\"\\n--- 8/8: Creating empty classifier dataset ---\")\n!mkdir -p /workspace/data/clf_data/imagesTr\n!mkdir -p /workspace/data/clf_data/labelsTr\n\nprint(\"\\n\\n✅✅✅ Final Preprocessing Complete! ✅✅✅\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add the local user 'bin' directory to the system PATH\n# This is where pip installs the nnU-Net executables\n%env PATH=$PATH:/root/.local/bin","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 4/8: Planning & Preprocessing Dataset 001 (RAM FIX) ---\nprint(\"\\n--- 4/8: Planning and preprocessing Dataset 001 (3d_fullres only) ---\")\n# We use -c 3d_fullres to skip the 2d config that crashes RAM\n!nnUNetv2_plan_and_preprocess -d 1 --verify_dataset_integrity -pl nnUNetPlannerResEncM -c 3d_fullres -np 2\nprint(\"✅ Preprocessing for Dataset 001 complete.\")\n\n# --- 5/8: Planning & Preprocessing Dataset 003 (CORRECT COMMAND) ---\nprint(\"\\n--- 5/8: Planning and preprocessing Dataset 003 ---\")\n# This is the correct command to create the '...ForcedLowres' plan\n!nnUNetv2_plan_and_preprocess -d 3 --verify_dataset_integrity -pl nnUNetPlannerResEncMForcedLowres -overwrite_target_spacing 1.0 1.0 1.0 -c 3d_fullres -np 2\nprint(\"✅ Planning and Preprocessing for Dataset 003 complete.\")\n\n# --- 6/8: JSON Patch (CORRECT FILE NAME) ---\nprint(\"\\n--- 6/8: Re-running JSON patch with correct file name ---\")\nplan_file = \"/workspace/data/nnUNet/nnUNet_preprocessed/Dataset003_VesselGrouping/nnUNetResEncUNetMPlans_ForcedLowres.json\"\n\ntry:\n    with open(plan_file, 'r') as f: data = json.load(f)\n    \n    data['plans_per_stage'][0]['patch_size'] = [64, 64, 64]\n    \n    with open(plan_file, 'w') as f: json.dump(data, f, indent=4)\n    print(\"✅ Patched nnUNet plans.json successfully.\")\n    \n    # 6.2 Re-preprocess Dataset 003 with the new patch size\n    print(\"\\n--- 6.2: Re-preprocessing Dataset 003 with new patch size ---\")\n    !nnUNetv2_preprocess -d 3 -plans_name nnUNetResEncUNetMPlans_ForcedLowres -c 3d_fullres -np 2\n    print(\"✅ Re-preprocessing for Dataset 003 complete.\")\nexcept Exception as e:\n    print(f\"❌ Error patching JSON. The file '{plan_file}' was not found. {e}\")\n\n# --- 7/8 & 8/8: Final Steps ---\nprint(\"\\n--- 7/8: Creating inference dataset ---\")\n!python src/nnUnet_utils/create_nnunet_inference_dataset.py\nprint(\"\\n--- 8/8: Creating empty classifier dataset ---\")\n!mkdir -p /workspace/data/clf_data/imagesTr\n!mkdir -p /workspace/data/clf_data/labelsTr\n\nprint(\"\\n\\n✅✅✅ Final Preprocessing Complete! ✅✅✅\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- STAGE 1: TRAINING THE nnU-Net SEGMENTATION MODELS ---\n# We are training only fold 0, for 15 epochs (which you set manually)\n\nprint(\"--- 1/4: Training Stage 1: Vessel Segmentation (Dataset 001, Fold 0) ---\")\n!nnUNetv2_train 1 3d_fullres 0 -p nnUNetPlannerResEncM -tr nnUNetTrainerSkeletonRecall_more_DAv3\n\nprint(\"\\n--- 2/4: Training Stage 1: Vessel Grouping (Dataset 003, Fold 0) ---\")\n!nnUNetv2_train 3 3d_fullres 0 -p nnUNetResEncUNetMPlans_ForcedLowres -tr RSNA2025Trainer_moreDAv7\n\n# --- STAGE 1: INFERENCE ---\n# Now we use the models we just trained to predict on our 100 scans.\n# This creates the ROI (Region of Interest) inputs for the next stage.\nprint(\"\\n--- 3/4: Running Stage 1 Inference (Creating ROIs) ---\")\n!python src/my_utils/vessel_segmentation.py\n\n# --- STAGE 2: TRAINING THE CLASSIFIER MODEL ---\n# This trains the final model. We override the epochs on the command line.\n# trainer.max_epochs=5 tells it to only train for 5 epochs (fast).\nprint(\"\\n--- 4/4: Training Stage 2: ROI Classifier (Fold 0, 5 Epochs) ---\")\n!python src/train.py \\\n  experiment=251013-seg_tf-v4-nnunet_truncate1-preV6_1-ex_dav6w3-m32g64-e25-w01_005_1-s128_256_256 \\\n  data.fold=0 \\\n  trainer.max_epochs=5\n\nprint(\"\\n\\n✅✅✅ Full Training Pipeline Complete! ✅✅✅\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 1/8: DICOM CONVERSION (ROBUST VERSION) ---\n# We will define the necessary functions right here in the notebook\n# to bypass all import/patching errors.\n\nimport SimpleITK as sitk\nimport pydicom\nimport joblib\nimport subprocess\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport os\n\nprint(\"--- 1/8: Converting DICOM to NIfTI ---\")\n\n# 1. Define the config (copied from the script)\nclass CFG:\n    img_dir = Path(\"/workspace/data/series\")\n    out_dir = Path(\"/workspace/data/series_niix\")\n    csv_dir = Path(\"/workspace/data\")\n    error_dir = Path(\"/workspace/data/error_data\")\n    num_workers = os.cpu_count()\n    \n# 2. Define the conversion functions (copied from the script)\ndef dcm2niix(\n    src_path: Path,\n    dst_path: Path,\n    series_id: str,\n) -> int:\n    \"\"\"Run dcm2niix\"\"\"\n    dst_path.mkdir(parents=True, exist_ok=True)\n    \n    # Command to run dcm2niix\n    cmd = [\n        \"dcm2niix\",\n        \"-o\", str(dst_path),\n        \"-f\", f\"{series_id}\",\n        \"-z\", \"y\",\n        \"-b\", \"n\",\n        \"-m\", \"2\",\n        str(src_path),\n    ]\n    \n    try:\n        # Run the command\n        subprocess.run(cmd, check=True, capture_output=True, text=True, timeout=300)\n    except subprocess.CalledProcessError as e:\n        # print(f\"Error running dcm2niix for {series_id}: {e.stderr}\")\n        return 1  # Error\n    except subprocess.TimeoutExpired:\n        # print(f\"Timeout running dcm2niix for {series_id}\")\n        return 1  # Error\n    return 0  # Success\n\ndef run_dcm2niix(series_ids: list[str]):\n    \"\"\"Run dcm2niix in parallel\"\"\"\n    print(f\"Running dcm2niix for {len(series_ids)} series...\")\n    \n    # Run in parallel\n    results = joblib.Parallel(n_jobs=CFG.num_workers)(\n        joblib.delayed(dcm2niix)(\n            src_path=CFG.img_dir / series_id,\n            dst_path=CFG.out_dir / series_id,\n            series_id=series_id,\n        )\n        for series_id in tqdm(series_ids, desc=\"Converting\")\n    )\n    \n    # Count successes and failures\n    success_count = sum(1 for res in results if res == 0)\n    failure_count = len(results) - success_count\n    print(f\"Done: {success_count} success, {failure_count} failure\")\n\n# 3. Run the conversion on OUR 100 scans\ntry:\n    # Load our 100-scan sample file\n    sample_df = pd.read_csv('/workspace/data/train.csv')\n    \n    # Get the list of 100 SeriesInstanceUIDs\n    series_ids_to_process = sample_df['SeriesInstanceUID'].tolist()\n    \n    print(f\"Found {len(series_ids_to_process)} scans to process.\")\n    \n    # Run the conversion\n    run_dcm2niix(series_ids_to_process)\n    \n    print(\"\\n✅ DICOM conversion finished.\")\n\nexcept Exception as e:\n    print(f\"❌ An error occurred during conversion: {e}\")\n    print(\"Please check that Step 4 ran correctly.\")\n\n\n# --- THE REST OF THE PIPELINE ---\n# The following steps are identical to before and will now work.\n\n# 2. Move any error data\nprint(\"\\n--- 2/8: Moving error data ---\")\n!python src/my_utils/move_error_data.py\n\n# 3. Create nnU-Net training dataset 1 (VesselSegmentation)\nprint(\"\\n--- 3/8: Creating nnU-Net Dataset 001 ---\")\n!python src/nnUnet_utils/create_nnunet_dataset.py\n# Create dataset 3 (VesselGrouping)\n!python src/nnUnet_utils/create_nnunet_dataset.py --dataset-id 3\n\n# 4. Run nnU-Net planning and preprocessing for Dataset 1\nprint(\"\\n--- 4/8: Planning and preprocessing Dataset 001 ---\")\n!nnUNetv2_plan_and_preprocess -d 1 --verify_dataset_integrity -pl nnUNetPlannerResEncM\n\n# 5. Run nnU-Net planning and preprocessing for Dataset 3\nprint(\"\\n--- 5/8: Planning and preprocessing Dataset 003 ---\")\n!nnUNetv2_plan_and_preprocess -d 3 --verify_dataset_integrity -pl nnUNetPlannerResEncMForcedLowres -overwrite_target_spacing 1.0 1.0 1.0 -c 3d_fullres\n\n# 6. Manually set patch size and re-preprocess (as per README)\nprint(\"\\n--- 6/8: Patching JSON and re-preprocessing Dataset 003 ---\")\nimport json\nplan_file = \"/workspace/data/nnUNet/nnUNet_preprocessed/Dataset003_VesselGrouping/nnUNetResEncUNetMPlans_ForcedLowres.json\"\n\ntry:\n    with open(plan_file, 'r') as f:\n        data = json.load(f)\n    \n    # Set the patch size as specified by the winner\n    data['plans_per_stage'][0]['patch_size'] = [128, 128, 128]\n    \n    with open(plan_file, 'w') as f:\n        json.dump(data, f, indent=4)\n    \n    print(\"✅ Patched nnUNet plans.json successfully.\")\n    \n    # 6.2 Re-preprocess\n    !nnUNetv2_preprocess -d 3 -plans_name nnUNetResEncUNetMPlans_ForcedLowres -c 3d_fullres\n\nexcept Exception as e:\n    print(f\"Error patching JSON. You may need to do this manually: {e}\")\n\n\n# 7. Create inference set\nprint(\"\\n--- 7/8: Creating inference dataset ---\")\n!python src/nnUnet_utils/create_nnunet_inference_dataset.py\n\n# 8. Create empty dataset for classifier (this is a small hack, but needed by the scripts)\nprint(\"\\n--- 8/8: Creating empty classifier dataset ---\")\n!mkdir -p /workspace/data/clf_data/imagesTr\n!mkdir -p /workspace/data/clf_data/labelsTr\n\nprint(\"\\n\\n✅✅✅ Preprocessing complete! ✅✅✅\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 0. FORCE-SET ENVIRONMENT VARIABLES ---\n# We do this again to be 100% safe\nprint(\"--- Force-setting nnU-Net Environment Variables ---\")\nimport os\nos.environ['nnUNet_raw'] = \"/workspace/data/nnUNet/nnUNet_raw\"\nos.environ['nnUNet_preprocessed'] = \"/workspace/data/nnUNet/nnUNet_preprocessed\"\nos.environ['nnUNet_results'] = \"/workspace/data/nnUNet/nnUNet_results\"\nprint(\"✅ Environment variables re-set.\")\n\n# --- 4/8: RE-RUNNING FAILED STEP 4 (with RAM fix) ---\nprint(\"\\n--- 4/8: Re-running preprocessing for Dataset 001 (3d_fullres only) ---\")\n# This time we manually tell it to ONLY process the '3d_fullres' config,\n# skipping the '2d' config that crashed the notebook.\n!nnUNetv2_preprocess -d 1 -c 3d_fullres -pl nnUNetPlannerResEncM -np 2\n\n# --- 5/8: \n# This is the CORRECT command from the original 60-min cell\n# It will create the '...ForcedLowres.json' file we need\n!nnUNetv2_plan_and_preprocess -d 3 --verify_dataset_integrity -pl nnUNetPlannerResEncMForcedLowres -overwrite_target_spacing 1.0 1.0 1.0 -c 3d_fullres -np 2\n\n# --- 6/8: RE-RUNNING FAILED STEP 6 (with correct file name) ---\nprint(\"\\n--- 6/8: Re-running JSON patch with correct file name ---\")\nimport json\n\n# This is the CORRECT file name from your log\nplan_file = \"/workspace/data/nnUNet/nnUNet_preprocessed/Dataset003_VesselGrouping/nnUNetResEncUNetMPlans.json\"\n\ntry:\n    with open(plan_file, 'r') as f:\n        data = json.load(f)\n    \n    # Set the patch size as specified by the winner\n    data['plans_per_stage'][0]['patch_size'] = [128, 128, 128]\n    \n    with open(plan_file, 'w') as f:\n        json.dump(data, f, indent=4)\n    \n    print(\"✅ Patched nnUNet plans.json successfully.\")\n    \n    # --- 6.2: Re-preprocess Dataset 003 with the new patch size ---\n    print(\"\\n--- 6.2: Re-preprocessing Dataset 003 with new patch size ---\")\n    # We must run this again so the new patch size is used.\n    !nnUNetv2_preprocess -d 3 -plans_name nnUNetResEncUNetMPlans -c 3d_fullres -np 2\n    print(\"✅ Re-preprocessing for Dataset 003 complete.\")\n\nexcept Exception as e:\n    print(f\"❌ Error patching JSON. This means the steps above failed again: {e}\")\n\n# 7. Create inference set\nprint(\"\\n--- 7/8: Creating inference dataset ---\")\n!python src/nnUnet_utils/create_nnunet_inference_dataset.py\n\n# 8. Create empty dataset for classifier (this is a small hack, but needed by the scripts)\nprint(\"\\n--- 8/8: Creating empty classifier dataset ---\")\n!mkdir -p /workspace/data/clf_data/imagesTr\n!mkdir -p /workspace/data/clf_data/labelsTr\n\nprint(\"\\n\\n✅✅✅ Final Preprocessing Complete! ✅✅✅\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 0. FORCE-SET ENVIRONMENT VARIABLES ---\n# This is the fix. We set the variables again in this new cell\n# so the following shell commands can find them.\nprint(\"--- Force-setting nnU-Net Environment Variables ---\")\nimport os\nos.environ['nnUNet_raw'] = \"/workspace/data/nnUNet/nnUNet_raw\"\nos.environ['nnUNet_preprocessed'] = \"/workspace/data/nnUNet/nnUNet_preprocessed\"\nos.environ['nnUNet_results'] = \"/workspace/data/nnUNet/nnUNet_results\"\n\n# Also export them for the shell commands\n!export nnUNet_raw=\"/workspace/data/nnUNet/nnUNet_raw\"\n!export nnUNet_preprocessed=\"/workspace/data/nnUNet/nnUNet_preprocessed\"\n!export nnUNet_results=\"/workspace/data/nnUNet/nnUNet_results\"\nprint(\"✅ Environment variables re-set.\")\n\n# Let's verify the datasets were created\nprint(\"\\n--- Verifying dataset locations ---\")\n!ls -l /workspace/data/nnUNet/nnUNet_raw/\nprint(\"---------------------------------\")\n\n\n# --- 4/8: RE-RUNNING FAILED STEP 4 ---\nprint(\"\\n--- 4/8: Re-running planning and preprocessing Dataset 001 ---\")\n!nnUNetv2_plan_and_preprocess -d 1 --verify_dataset_integrity -pl nnUNetPlannerResEncM\n\n# --- 5/8: RE-RUNNING FAILED STEP 5 ---\nprint(\"\\n--- 5/8: Re-running planning and preprocessing Dataset 003 ---\")\n!nnUNetv2_plan_and_preprocess -d 3 --verify_dataset_integrity -pl nnUNetPlannerResEncMForcedLowres -overwrite_target_spacing 1.0 1.0 1.0 -c 3d_fullres\n\n# --- 6/8: RE-RUNNING FAILED STEP 6 ---\nprint(\"\\n--- 6/8: Re-running JSON patch and re-preprocessing ---\")\nimport json\nplan_file = \"/workspace/data/nnUNet/nnUNet_preprocessed/Dataset003_VesselGrouping/nnUNetResEncUNetMPlans_ForcedLowres.json\"\n\ntry:\n    with open(plan_file, 'r') as f:\n        data = json.load(f)\n    \n    # Set the patch size as specified by the winner\n    data['plans_per_stage'][0]['patch_size'] = [128, 128, 128]\n    \n    with open(plan_file, 'w') as f:\n        json.dump(data, f, indent=4)\n    \n    print(\"✅ Patched nnUNet plans.json successfully.\")\n    \n    # 6.2 Re-preprocess\n    !nnUNetv2_preprocess -d 3 -plans_name nnUNetResEncUNetMPlans_ForcedLowres -c 3d_fullres\n    print(\"✅ Re-preprocessing for Dataset 003 complete.\")\n\nexcept Exception as e:\n    print(f\"❌ Error patching JSON. This means the steps above failed again: {e}\")\n\nprint(\"\\n\\n✅✅✅ Preprocessing Fix Complete! ✅✅✅\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 0. FORCE-SET ENVIRONMENT VARIABLES ---\n# We do this again to be 100% safe\nprint(\"--- Force-setting nnU-Net Environment Variables ---\")\nimport os\nos.environ['nnUNet_raw'] = \"/workspace/data/nnUNet/nnUNet_raw\"\nos.environ['nnUNet_preprocessed'] = \"/workspace/data/nnUNet/nnUNet_preprocessed\"\nos.environ['nnUNet_results'] = \"/workspace/data/nnUNet/nnUNet_results\"\nprint(\"✅ Environment variables re-set.\")\n\n# --- 4/8: RE-RUNNING FAILED STEP 4 (with RAM fix) ---\nprint(\"\\n--- 4/8: Re-running planning and preprocessing Dataset 001 ---\")\n# We add -np 2 to use only 2 CPU cores, preventing the RAM crash.\n!nnUNetv2_plan_and_preprocess -d 1 --verify_dataset_integrity -pl nnUNetPlannerResEncM -np 2\n\n# --- 5/8: RE-RUNNING FAILED STEP 5 (with correct command) ---\nprint(\"\\n--- 5/8: Re-running planning and preprocessing Dataset 003 ---\")\n# This is the CORRECT command from the original 60-min cell\n# It will create the '...ForcedLowres.json' file we need\n!nnUNetv2_plan_and_preprocess -d 3 --verify_dataset_integrity -pl nnUNetPlannerResEncMForcedLowres -overwrite_target_spacing 1.0 1.0 1.0 -c 3d_fullres -np 2\n\n# --- 6/8: RE-RUNNING FAILED STEP 6 (will now work) ---\nprint(\"\\n--- 6/8: Re-running JSON patch and re-preprocessing ---\")\nimport json\n# This file will now exist\nplan_file = \"/workspace/data/nnUNet/nnUNet_preprocessed/Dataset003_VesselGrouping/nnUNetResEncUNetMPlans_ForcedLowres.json\"\n\ntry:\n    with open(plan_file, 'r') as f:\n        data = json.load(f)\n    \n    # Set the patch size as specified by the winner\n    data['plans_per_stage'][0]['patch_size'] = [128, 128, 128]\n    \n    with open(plan_file, 'w') as f:\n        json.dump(data, f, indent=4)\n    \n    print(\"✅ Patched nnUNet plans.json successfully.\")\n    \n    # 6.2 Re-preprocess\n    !nnUNetv2_preprocess -d 3 -plans_name nnUNetResEncUNetMPlans_ForcedLowres -c 3d_fullres -np 2\n    print(\"✅ Re-preprocessing for Dataset 003 complete.\")\n\nexcept Exception as e:\n    print(f\"❌ Error patching JSON. This means the steps above failed again: {e}\")\n\nprint(\"\\n\\n✅✅✅ Preprocessing Fix Complete! ✅✅✅\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
